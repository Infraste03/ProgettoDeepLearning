
"""
This script trains or tests a Wasserstein GAN with Gradient Penalty (WGAN-GP) on the CelebA dataset.

The main components of the script are:
1. **Data Loading**: The `get_loader` function is used to create a data loader for the CelebA dataset, with attributes such as 'Smiling', 'Young', 'Male', etc. The dataset is loaded in batches and prepared for training.

2. **Model Definition**: The `Generator` and `Discriminator` classes are used to define the generator and discriminator networks. The generator creates fake images, while the discriminator evaluates the authenticity of real and fake images.

3. **Gradient Penalty**: A function `gradient_penalty` calculates the gradient penalty to enforce the Lipschitz constraint required by WGAN-GP. This penalty helps stabilize training by penalizing deviations in the gradients.

4. **Training Loop**: The script trains the WGAN-GP for a specified number of epochs. The discriminator is updated multiple times per generator update, with each update involving real and fake data along with the gradient penalty. The generator is updated based on the discriminator's feedback.

5. **Logging and Visualization**: TensorBoard is used for logging metrics such as losses and gradient histograms. Sample images generated by the generator are saved periodically to monitor progress.

6. **Model Saving**: The trained models are saved every 5 epochs, and sample images generated by the current model are also saved.

7. **Testing**: If the script is run with the 'test' argument, it will call the `test()` function, which is presumably defined in the `TESTWGANGRADIENT` module for evaluating the trained models.

To use this script, run it with either 'train' or 'test' as an argument, depending on your desired operation.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data
import torchvision.utils as vutils
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torch.utils import data
from torchvision import transforms as T
from dataloaderWGAN import get_loader
from generatorWGAN import Generator
from discriminatorWGAN import Discriminator
import argparse
from TESTWGANGRADIENT import test

from torch.utils.tensorboard import SummaryWriter
from matplotlib.lines import Line2D

# Parameters
dataroot = 'C:/Users/fstef/Desktop/PROGETTO_PRATI/celeba/images'
attr_path = 'C:/Users/fstef/Desktop/PROGETTO_PRATI/celeba/list_attr_celeba.txt'
selected_attrs = ['Smiling', 'Young', 'Male', 'Attractive','Big_Lips', 
                  'Black_Hair','Eyeglasses', 'Blond_Hair','5_o_Clock_Shadow','Arched_Eyebrows','Attractive']
batch_size = 128 #256
image_size = 64
nz = 128
ngf = 64
ndf = 64
num_epochs = 51
lr = 0.0001 #0.0001
beta1 = 0.7 #0.5
ngpu = 1
label_dim = len(selected_attrs)
critic_iters = 10  # Number of Critic iterations per Generator iteration
lambda_gp = 10  # Gradient penalty coefficient


writer = SummaryWriter('./runs/tensorWGAN2NEW')

# Gradient penalty function
def gradient_penalty(netD, real_data, fake_data, labels):
    """
    The function calculates the gradient penalty for improving the training of a discriminator in a GAN
    by penalizing the norm of the gradients of the discriminator's output with respect to interpolated
    real and fake data points.
    
    :param netD: The `netD` parameter in the `gradient_penalty` function is typically a discriminator
    neural network that takes input data (real or fake) and predicts whether it is real or fake. This
    discriminator network is used in the context of training a Generative Adversarial Network (GAN)
    where it is
    :param real_data: Real data samples from the dataset
    :param fake_data: Fake data generated by the generator network in a GAN (Generative Adversarial
    Network) during training
    :param labels: Labels are the target labels or classes associated with the data samples. In the
    context of the `gradient_penalty` function provided, labels are used as input to the discriminator
    network `netD` along with the interpolated data (a mix of real and fake data) to compute the output
    for the interpolated samples
    :return: The function `gradient_penalty` returns the gradient penalty calculated based on the input
    real data, fake data, and labels using the Wasserstein GAN gradient penalty formula.
    """
    batch_size = real_data.size(0)
    epsilon = torch.rand(batch_size, 1, 1, 1, device=real_data.device)
    interpolated = epsilon * real_data + (1 - epsilon) * fake_data
    interpolated = interpolated.requires_grad_(True)

    output_interpolated = netD(interpolated, labels)

    gradients = torch.autograd.grad(
        outputs=output_interpolated,
        inputs=interpolated,
        grad_outputs=torch.ones(output_interpolated.size(), device=real_data.device),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    gradients = gradients.view(gradients.size(0), -1)
    gradient_norm = gradients.norm(2, dim=1)
    gradient_penalty = ((gradient_norm - 1) ** 2).mean()

    return gradient_penalty

if __name__ == '__main__':
    # Create the dataset and dataloader using the new method
    parser = argparse.ArgumentParser(description="TRAIN OR TEST ?")
    parser.add_argument('operation', choices=['train', 'test'], help="TRAIN OR TEST?")
    args = parser.parse_args()

    if args.operation == 'train':
    
    
        # The above code snippet is creating a data loader object for training a model. It is using
        # the `get_loader` function with parameters such as `dataroot` (root directory of the
        # dataset), `attr_path` (path to the attribute file), `selected_attrs` (list of selected
        # attributes), `image_size` (size of the input images), `batch_size` (number of samples in
        # each batch), `mode='train'` (indicating that it is for training), and `num_workers=2`
        # (number of subprocesses to use for data loading). This data loader
        dataloader = get_loader(dataroot, attr_path, selected_attrs, image_size=image_size, 
                                batch_size=batch_size, mode='train', num_workers=2)

        # Decide which device we want to run on
        device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")
        print("Using device:", device)

        real_batch = next(iter(dataloader))
        plt.figure(figsize=(8,8))
        plt.axis("off")
        plt.title("Training Images")
        plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))
        #plt.show()
        
        ##########################
        img_grid = vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu()
        img_grid = img_grid.numpy()
        img_grid = np.transpose(img_grid, (1, 2, 0))  # Convert from [C, H, W] to [H, W, C]
        # Add the image to TensorBoard
        writer.add_image('Example Star', img_grid, dataformats='HWC')
        
        def add_gradient_hist(net):
            ave_grads = [] 
            layers = []
            for n,p in net.named_parameters():
                if ("bias" not in n):
                    layers.append(n)
                    if p.requires_grad: 
                        ave_grad = np.abs(p.grad.clone().detach().cpu().numpy()).mean()
                    else:
                        ave_grad = 0
                    ave_grads.append(ave_grad)
                
            layers = [layers[i].replace(".weight", "") for i in range(len(layers))]
            
            fig = plt.figure(figsize=(12, 12))
            plt.bar(np.arange(len(ave_grads)), ave_grads, lw=1, color="b")
            plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color="k")
            plt.xticks(range(0, len(ave_grads), 1), layers, rotation=90)
            plt.xlim(left=0, right=len(ave_grads))
            plt.ylim(bottom=-0.001, top=np.max(ave_grads) / 2)  # zoom in on the lower gradient regions
            plt.xlabel("Layers")
            plt.ylabel("average gradient")
            plt.title("Gradient flow")
            #plt.grid(True)
            plt.legend([Line2D([0], [0], color="b", lw=4),
                        Line2D([0], [0], color="k", lw=4)], ['mean-gradient', 'zero-gradient'])
            plt.tight_layout()
            #plt.show()
            
            return fig
        
        netG = Generator(nz,ngf,label_dim).to(device)
        netD = Discriminator(ndf,label_dim).to(device)

        # Apply the weights_init function to randomly initialize all weights
        def weights_init(m):
            """
            The function `weights_init` initializes weights and biases of Convolutional and Batch
            Normalization layers in a neural network.
            
            :param m: The `weights_init` function you provided is used to initialize the weights of a
            neural network model. The function takes a module `m` as input, which represents a layer or a
            block in the neural network model
            """
            classname = m.__class__.__name__
            if classname.find('Conv') != -1:
                nn.init.normal_(m.weight.data, 0.0, 0.02)
            elif classname.find('BatchNorm') != -1:
                nn.init.normal_(m.weight.data, 1.0, 0.02)
                nn.init.constant_(m.bias.data, 0)
                
                
        """ 
        def weights_init(m):
            classname = m.__class__.__name__
            if classname.find('Conv') != -1:
                # Initialize Conv layers with Xavier initialization
                nn.init.xavier_normal_(m.weight, gain=nn.init.calculate_gain('relu'))
            elif classname.find('BatchNorm') != -1:
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif classname.find('Linear') != -1:
                # Initialize Linear layers with Xavier initialization, if present
                nn.init.xavier_normal_(m.weight, gain=nn.init.calculate_gain('relu'))
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
                    
            def weights_init(m):
            classname = m.__class__.__name__
            if classname.find('Conv') != -1:
                # Usa Kaiming per layer di convoluzione con ReLU
                nn.init.kaiming_normal_(m.weight.data, mode='fan_in', nonlinearity='relu')
            elif classname.find('Linear') != -1:
                # Usa Xavier per layer lineari
                nn.init.xavier_normal_(m.weight.data, gain=nn.init.calculate_gain('relu'))
            elif classname.find('BatchNorm') != -1:
                # Inizializza BatchNorm: setta i pesi a 1 e i bias a 0
                if hasattr(m, 'weight') and m.weight is not None:
                    nn.init.constant_(m.weight.data, 1)
                if hasattr(m, 'bias') and m.bias is not None:
                    nn.init.constant_(m.bias.data, 0)
            elif classname.find('LayerNorm') != -1:
                # Usa Xavier per LayerNorm
                nn.init.xavier_normal_(m.weight.data, gain=nn.init.calculate_gain('relu'))"""

        netG.apply(weights_init)
        netD.apply(weights_init)

        # Setup optimizer
        
        optimizerD = optim.RMSprop(netD.parameters(), lr=lr)
        optimizerG = optim.RMSprop(netG.parameters(), lr=lr)


        # Training Loop
        print("Starting Training Loop...")
        for epoch in range(num_epochs):
            for i, data in enumerate(dataloader, 0):
                real, labels = data
                batch_size = real.size(0)
                real = real.to(device)
                labels = torch.cat([labels] * (image_size // 64), dim=0).unsqueeze(2).unsqueeze(3).expand(-1, -1, image_size, image_size).to(device)

                # Update Discriminator (Critic)
                
                for p in netD.parameters():
                    p.requires_grad = True

                for _ in range(critic_iters):
                    netD.zero_grad()
                    output_real = netD(real, labels)
                    errD_real = output_real.mean()

                    noise = torch.randn(batch_size, nz, 1, 1, device=device)
                    fake = netG(noise, labels)
                    output_fake = netD(fake.detach(), labels)
                    errD_fake = output_fake.mean()

                    # Gradient penalty
                    gp = gradient_penalty(netD, real, fake, labels)

                    # Combined loss
                    D_loss = errD_fake - errD_real + lambda_gp * gp
                    D_loss.backward(retain_graph=True)  # Retain the graph
                    optimizerD.step()

                D_x = errD_real.item()
                D_G_z1 = errD_fake.item()

                # Update Generator
                for p in netD.parameters():
                    p.requires_grad = False
                netG.zero_grad()
                output = netD(fake, labels)
                G_loss = -output.mean()
                G_loss.backward(retain_graph=True)  # Retain the graph
                optimizerG.step()

                writer.add_figure('gradients',
                                add_gradient_hist(netG),
                                global_step=epoch * len(dataloader) + i)
                writer.add_figure('gradients',
                                add_gradient_hist(netD),
                                global_step=epoch * len(dataloader) + i)

                print(f'Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(dataloader)}] '
                f'Discriminator Loss: {D_loss.item():.4f} '
                f'Wasserstein Loss: {errD_real.item() - errD_fake.item():.4f} '
                f'Gradient Penalty: {gp.item():.4f}')
                
                print(f'Generator Loss: {G_loss.item():.4f}')
                
                writer.add_scalar('D(x)', D_x, epoch * len(dataloader) + i)
                writer.add_scalar('D(G(z))', D_G_z1, epoch * len(dataloader) + i)


            
            if(epoch % 5 == 0):
            # Save the models
                torch.save(netG.state_dict(), f'generatorHPCWGANGRADEINT_{epoch+1}.pth')
                torch.save(netD.state_dict(), f'discriminatorHPCWGANGRADEINT_{epoch+1}.pth')

                # Save some samples
                with torch.no_grad():
                    fixed_noise = torch.randn(64, nz, 1, 1, device=device)
                    fixed_labels = torch.zeros(64, label_dim, image_size, image_size, device=device)
                    fake = netG(fixed_noise, fixed_labels)
                    vutils.save_image(fake.detach(), f'imagesHPCWGANGRADEINT_{epoch+1}.png', normalize=True)
                    
                    fake_image = fake[0]
                
                    fake_image = fake_image.cpu().numpy()
                    fake_image = np.transpose(fake_image, (1, 2, 0))  # Convert from [C, H, W] to [H, W, C]

                    # Add the image to TensorBoard
                    
                    writer.add_image('FAKE WGANGRADIENT Star', fake_image, dataformats='HWC')
            
            print ("FINE DEL TRAINING : ) ")
            
            
    # The code snippet is checking if the value of `args.operation` is equal to the string 'test'. If
    # it is, then it calls the function `test()`. 
    elif args.operation == 'test':
        test()